{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 180 Machine Project: Global Temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project aims to find an effective machine learning algorithm that can predict whether a Twitter user supports the belief in man-made climate change.\n",
    "\n",
    "### Features\n",
    "The features to be extracted are:\n",
    "<br>\n",
    "* Unigrams\n",
    "* Bigrams\n",
    "* Trigrams\n",
    "<br>\n",
    "\n",
    "### Model\n",
    "The models to be used are:\n",
    "<br>\n",
    "* Support Vector Machine\n",
    "* Decision Trees\n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to load the libraries to be used in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "The Twitter data comes from two datasets:\n",
    "<br>\n",
    "* Kaggle\n",
    "* data.world\n",
    "<br>\n",
    "\n",
    "The Twitter sentiments needed are:\n",
    "<br>\n",
    "1. Pro\n",
    "2. Anti\n",
    "3. Neutral \n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"../data/kaggle_twitter_data.csv\")\n",
    "data2 = pd.read_csv(\"../data/dataworld_twitter_data.csv\")\n",
    "\n",
    "# Remove other columns\n",
    "data1 = data1[[\"sentiment\", \"tweet\"]]\n",
    "data2 = data2[[\"sentiment\", \"tweet\"]]\n",
    "\n",
    "# Remove sentiment=2 from Kaggle data set\n",
    "data1 = data1[data1.sentiment != 2]\n",
    "\n",
    "frames = [data1, data2]\n",
    "\n",
    "# All data\n",
    "data = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The data (the Tweet) is preprocessed such that:\n",
    "1. Rows with NAN are dropped\n",
    "2. Tweet is converted to string\n",
    "3. Tweets are lowercased\n",
    "4. Removes non-English Characters from the Tweet\n",
    "5. Removes URLs from the Tweet\n",
    "6. Removing RT and hyperlink from Tweet\n",
    "7. Removes @ and # from Tweet\n",
    "8. Remove stopwords from Tweet\n",
    "9. Remove numbers from Tweet\n",
    "10. Remove punctuations from Tweet\n",
    "11. The Tweet is tokenized\n",
    "12. The Tweet is lemmatized\n",
    "<br>\n",
    "\n",
    "After that, the preprocessed tweet is a new column to the dataset\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# Drop rows with NA or NAN\n",
    "data = data.dropna()\n",
    "\n",
    "df = data[\"tweet\"]\n",
    "\n",
    "# Make tweet to str\n",
    "df = df.apply(str)\n",
    "\n",
    "# Lowercase all words\n",
    "df = df.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove non-English characters\n",
    "df = df.apply(lambda x: x.encode(\"ascii\", \"ignore\").decode())\n",
    "\n",
    "# Remove URLS\n",
    "df = df.apply(\n",
    "    lambda x: re.sub(r\"http?://[A-Za-z0-9./]+\", \"\", x, flags=re.MULTILINE)\n",
    ")\n",
    "df = df.apply(\n",
    "    lambda x: re.sub(r\"https?://[A-Za-z0-9./]+\", \"\", x, flags=re.MULTILINE)\n",
    ")\n",
    "df = df.apply(\n",
    "    lambda x: re.sub(r\"www?://[A-Za-z0-9./]+\", \"\", x, flags=re.MULTILINE)\n",
    ")\n",
    "\n",
    "# Removing RT and link\n",
    "df = df.apply(lambda x: re.sub(r\"\\bRT\\b\", \"\", x).strip())\n",
    "df = df.apply(lambda x: re.sub(r\"\\blink\\b\", \"\", x).strip())\n",
    "\n",
    "# Remove @ and #\n",
    "df = df.apply(lambda x: re.sub(r\"@[A-Za-z0-9_]+\", \"\", x))\n",
    "df = df.apply(lambda x: re.sub(r\"#[A-Za-z0-9_]+\", \"\", x))\n",
    "\n",
    "# Remove stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "df = df.apply(lambda x: \" \".join([x for x in x.split() if x not in stop]))\n",
    "\n",
    "# Remove numbers\n",
    "df = df.apply(lambda x: re.sub(r\"[0-9]+\", \"\", x))\n",
    "\n",
    "# Remove punctuations\n",
    "df = df.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Tokenized\n",
    "tokenizedTweets = [word_tokenize(x) for x in df]\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for tweet in tokenizedTweets:\n",
    "    for word in tweet:\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "processed = tokenizedTweets\n",
    "\n",
    "# Append changed tweet to database\n",
    "final = []\n",
    "for x in range(len(processed)):\n",
    "    final.append(\" \".join(processed[x]))\n",
    "\n",
    "out = pd.DataFrame(final)\n",
    "\n",
    "data[\"changedtweet\"] = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting of Data\n",
    "\n",
    "The data is split into three categories:\n",
    "<br>\n",
    "* Positive\n",
    "* Negative\n",
    "* Neutral\n",
    "<br>\n",
    "\n",
    "The first 5000 tweets are to be used for testing and training.\n",
    "\n",
    "For each category, the samples are split such that there are 60% tweets for training and 40% tweets for testing.\n",
    "Therefore, for each category, there are\n",
    "<br>\n",
    "* 3000 tweets for training\n",
    "* 2000 tweets for testing\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5000 samples per label\n",
    "positive = data[data[\"sentiment\"] == 1][:5000]\n",
    "negative = data[data[\"sentiment\"] == -1][:5000]\n",
    "neutral = data[data[\"sentiment\"] == 0][:5000]\n",
    "\n",
    "#The features are extracted from the changed tweet and the target is the sentiment\n",
    "features = [\"changedtweet\"]\n",
    "targets = [\"sentiment\"]\n",
    "\n",
    "#Create an empty Dataframe for features in train and test and sentiment in train and test\n",
    "X_train = pd.DataFrame(columns = features)\n",
    "X_test = pd.DataFrame(columns = features)\n",
    "y_train = pd.DataFrame(columns = targets)\n",
    "y_test = pd.DataFrame(columns = targets)\n",
    "\n",
    "#Set an empty array to append the tweets\n",
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for category in (positive, negative, neutral):\n",
    "    X = category[\"changedtweet\"]\n",
    "    y = category[\"sentiment\"]\n",
    "    Xs_train, Xs_test, ys_train, ys_test = train_test_split(X, y, random_state=0, train_size=0.6) #Split the dataset\n",
    "    \n",
    "    #Append the split data set to the array\n",
    "    X_train_list.append(Xs_train)\n",
    "    X_test_list.append(Xs_test)\n",
    "    y_train_list.append(ys_train)\n",
    "    y_test_list.append(ys_test)\n",
    "\n",
    "#Concat the three categories to create one dataset\n",
    "X_train = pd.concat(X_train_list, ignore_index=True)\n",
    "X_test = pd.concat(X_test_list, ignore_index=True)\n",
    "y_train = pd.concat(y_train_list, ignore_index=True)\n",
    "y_test = pd.concat(y_test_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction (Unigrams)\n",
    "\n",
    "To extract the feature, use TF-IDF vectorizer.\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to vectorize the tweets (for the Unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To extract the features use TF-IDF vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(ngram_range=(1,1)) #Unigram\n",
    "Tfidf_vect.fit(data['changedtweet']) #Fit the vectorizer to the pre-processed tweets\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train) #Get the vectorized tweets for trainig\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test) #Get the vectorized tweets for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Algorithm for Unigrams\n",
    "\n",
    "The SVM is set up with the hyperparameters:\n",
    "<br>\n",
    "* Kernel- Radial Basis Function\n",
    "* Gamma- 1.3\n",
    "* C- 1000\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report for Unigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.72      0.74      2000\n",
      "           0       0.65      0.78      0.71      2000\n",
      "           1       0.90      0.77      0.83      2000\n",
      "\n",
      "    accuracy                           0.76      6000\n",
      "   macro avg       0.77      0.76      0.76      6000\n",
      "weighted avg       0.77      0.76      0.76      6000\n",
      "\n",
      "SVM Accuracy Score for Unigrams-> 75.6333\n",
      "SVM Precision Score for Unigrams-> 75.8217\n",
      "SVM Recall Score for Unigrams-> 75.6333\n",
      "SVM F1 Score for Unigrams-> 75.3047\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(kernel='rbf', gamma=1.3, C=1000)\n",
    "SVM.fit(Train_X_Tfidf,y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "print(\"SVM Classification Report for Unigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_SVM))\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score for Unigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_SVM, y_test)*100,4))\n",
    "print(\"SVM Precision Score for Unigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM Recall Score for Unigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM F1 Score for Unigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_SVM, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm for Unigrams\n",
    "\n",
    "The Decision Tree is set up with the hyperparameters:\n",
    "<br>\n",
    "* random state- 0\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report for Unigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.64      0.55      0.59      2000\n",
      "           0       0.55      0.67      0.61      2000\n",
      "           1       0.75      0.71      0.73      2000\n",
      "\n",
      "    accuracy                           0.64      6000\n",
      "   macro avg       0.65      0.64      0.64      6000\n",
      "weighted avg       0.65      0.64      0.64      6000\n",
      "\n",
      "Decision Tree Accuracy Score for Unigrams-> 64.2167\n",
      "Decision Tree Precision Score for Unigrams-> 64.6938\n",
      "Decision Tree Recall Score for Unigrams-> 64.2167\n",
      "Decision Tree F1 Score for Unigrams-> 64.1277\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - Decision Tree\n",
    "# fit the training dataset on the classifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(Train_X_Tfidf, y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_DecisionTree = dt.predict(Test_X_Tfidf)\n",
    "print(\"Decision Tree Classification Report for Unigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_DecisionTree))\n",
    "\n",
    "print(\"Decision Tree Accuracy Score for Unigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_DecisionTree, y_test)*100,4))\n",
    "print(\"Decision Tree Precision Score for Unigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree Recall Score for Unigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree F1 Score for Unigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction (Bigrams)\n",
    "\n",
    "To extract the feature, use TF-IDF vectorizer.\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to vectorize the tweets (for the Bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To extract the features use TF-IDF vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(ngram_range=(2,2)) #Unigram\n",
    "Tfidf_vect.fit(data['changedtweet']) #Fit the vectorizer to the pre-processed tweets\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train) #Get the vectorized tweets for trainig\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test) #Get the vectorized tweets for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Algorithm for Bigrams\n",
    "\n",
    "The SVM is set up with the hyperparameters:\n",
    "<br>\n",
    "* Kernel- Radial Basis Function\n",
    "* Gamma- 1.3\n",
    "* C- 1000\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report for Bigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      0.68      0.67      2000\n",
      "           0       0.55      0.72      0.63      2000\n",
      "           1       0.97      0.63      0.76      2000\n",
      "\n",
      "    accuracy                           0.68      6000\n",
      "   macro avg       0.73      0.68      0.69      6000\n",
      "weighted avg       0.73      0.68      0.69      6000\n",
      "\n",
      "SVM Accuracy Score for Bigrams-> 67.9\n",
      "SVM Precision Score for Bigrams-> 68.9252\n",
      "SVM Recall Score for Bigrams-> 67.9\n",
      "SVM F1 Score for Bigrams-> 67.1229\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(kernel='rbf', gamma=1.3, C=1000)\n",
    "SVM.fit(Train_X_Tfidf,y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "print(\"SVM Classification Report for Bigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_SVM))\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score for Bigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_SVM, y_test)*100,4))\n",
    "print(\"SVM Precision Score for Bigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM Recall Score for Bigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM F1 Score for Bigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_SVM, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm for Bigrams\n",
    "\n",
    "The Decision Tree is set up with the hyperparameters:\n",
    "<br>\n",
    "* random state- 0\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report for Bigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.57      0.57      2000\n",
      "           0       0.51      0.61      0.56      2000\n",
      "           1       0.81      0.66      0.73      2000\n",
      "\n",
      "    accuracy                           0.61      6000\n",
      "   macro avg       0.63      0.61      0.62      6000\n",
      "weighted avg       0.63      0.61      0.62      6000\n",
      "\n",
      "Decision Tree Accuracy Score for Bigrams-> 61.25\n",
      "Decision Tree Precision Score for Bigrams-> 60.973\n",
      "Decision Tree Recall Score for Bigrams-> 61.25\n",
      "Decision Tree F1 Score for Bigrams-> 60.7093\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - Decision Tree\n",
    "# fit the training dataset on the classifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(Train_X_Tfidf, y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_DecisionTree = dt.predict(Test_X_Tfidf)\n",
    "print(\"Decision Tree Classification Report for Bigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_DecisionTree))\n",
    "\n",
    "print(\"Decision Tree Accuracy Score for Bigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_DecisionTree, y_test)*100,4))\n",
    "print(\"Decision Tree Precision Score for Bigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree Recall Score for Bigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree F1 Score for Bigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_DecisionTree, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction (Trigrams)\n",
    "\n",
    "To extract the feature, use TF-IDF vectorizer.\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to vectorize the tweets (for the Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To extract the features use TF-IDF vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(ngram_range=(3,3)) #Unigram\n",
    "Tfidf_vect.fit(data['changedtweet']) #Fit the vectorizer to the pre-processed tweets\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train) #Get the vectorized tweets for trainig\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test) #Get the vectorized tweets for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Algorithm for Trigrams\n",
    "\n",
    "The SVM is set up with the hyperparameters:\n",
    "<br>\n",
    "* Kernel- Radial Basis Function\n",
    "* Gamma- 1.3\n",
    "* C- 1000\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report for Trigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.63      0.55      0.59      2000\n",
      "           0       0.51      0.78      0.62      2000\n",
      "           1       0.98      0.59      0.73      2000\n",
      "\n",
      "    accuracy                           0.64      6000\n",
      "   macro avg       0.71      0.64      0.65      6000\n",
      "weighted avg       0.71      0.64      0.65      6000\n",
      "\n",
      "SVM Accuracy Score for Trigrams-> 64.0\n",
      "SVM Precision Score for Trigrams-> 67.4366\n",
      "SVM Recall Score for Trigrams-> 64.0\n",
      "SVM F1 Score for Trigrams-> 63.2697\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(kernel='rbf', gamma=1.3, C=1000)\n",
    "SVM.fit(Train_X_Tfidf,y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "print(\"SVM Classification Report for Trigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_SVM))\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score for Trigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_SVM, y_test)*100,4))\n",
    "print(\"SVM Precision Score for Trigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM Recall Score for Trigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM F1 Score for Trigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_SVM, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm for Trigrams\n",
    "\n",
    "The Decision Tree is set up with the hyperparameters:\n",
    "<br>\n",
    "* random state- 0\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report for Trigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.39      0.50      2000\n",
      "           0       0.50      0.78      0.61      2000\n",
      "           1       0.82      0.72      0.77      2000\n",
      "\n",
      "    accuracy                           0.63      6000\n",
      "   macro avg       0.67      0.63      0.63      6000\n",
      "weighted avg       0.67      0.63      0.63      6000\n",
      "\n",
      "Decision Tree Accuracy Score for Trigrams-> 63.0167\n",
      "Decision Tree Precision Score for Trigrams-> 69.0803\n",
      "Decision Tree Recall Score for Trigrams-> 63.0167\n",
      "Decision Tree F1 Score for Trigrams-> 63.5244\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - Decision Tree\n",
    "# fit the training dataset on the classifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(Train_X_Tfidf, y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_DecisionTree = dt.predict(Test_X_Tfidf)\n",
    "print(\"Decision Tree Classification Report for Trigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_DecisionTree))\n",
    "\n",
    "print(\"Decision Tree Accuracy Score for Trigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_DecisionTree, y_test)*100,4))\n",
    "print(\"Decision Tree Precision Score for Trigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree Recall Score for Trigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree F1 Score for Trigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_DecisionTree, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
