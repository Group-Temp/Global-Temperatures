{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 180 Machine Project: Global Temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project aims to find an effective machine learning algorithm that can predict whether a Twitter user supports the belief in man-made climate change.\n",
    "\n",
    "### Features\n",
    "The features to be extracted are:\n",
    "<br>\n",
    "* Unigrams\n",
    "* Bigrams\n",
    "* Trigrams\n",
    "<br>\n",
    "\n",
    "### Model\n",
    "The models to be used are:\n",
    "<br>\n",
    "* Support Vector Machine\n",
    "* Decision Trees\n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to load the libraries to be used in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "The Twitter data comes from two datasets:\n",
    "<br>\n",
    "* Kaggle\n",
    "* data.world\n",
    "<br>\n",
    "\n",
    "The Twitter sentiments needed are:\n",
    "<br>\n",
    "1. Pro\n",
    "2. Anti\n",
    "3. Neutral \n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"../data/kaggle_twitter_data.csv\")\n",
    "data2 = pd.read_csv(\"../data/dataworld_twitter_data.csv\")\n",
    "\n",
    "# Remove other columns\n",
    "data1 = data1[[\"sentiment\", \"tweet\"]]\n",
    "data2 = data2[[\"sentiment\", \"tweet\"]]\n",
    "\n",
    "# Remove sentiment=2 from Kaggle data set\n",
    "data1 = data1[data1.sentiment != 2]\n",
    "\n",
    "frames = [data1, data2]\n",
    "\n",
    "# All data\n",
    "data = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The data (the Tweet) is preprocessed such that:\n",
    "1. Rows with NAN are dropped\n",
    "2. Tweet is converted to string\n",
    "3. Tweets are lowercased\n",
    "4. Removes non-English Characters from the Tweet\n",
    "5. Removes URLs from the Tweet\n",
    "6. Removing RT and hyperlink from Tweet\n",
    "7. Removes @ and # from Tweet\n",
    "8. Remove stopwords from Tweet\n",
    "9. Remove numbers from Tweet\n",
    "10. Remove punctuations from Tweet\n",
    "11. The Tweet is tokenized\n",
    "12. The Tweet is lemmatized\n",
    "<br>\n",
    "\n",
    "After that, the preprocessed tweet is a new column to the dataset\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# Drop rows with NA or NAN\n",
    "data = data.dropna()\n",
    "\n",
    "df = data[\"tweet\"]\n",
    "\n",
    "# Make tweet to str\n",
    "df = df.apply(str)\n",
    "\n",
    "# Lowercase all words\n",
    "df = df.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove non-English characters\n",
    "df = df.apply(lambda x: x.encode(\"ascii\", \"ignore\").decode())\n",
    "\n",
    "# Remove URLS\n",
    "df = df.apply(\n",
    "    lambda x: re.sub(r\"http?://[A-Za-z0-9./]+\", \"\", x, flags=re.MULTILINE)\n",
    ")\n",
    "df = df.apply(\n",
    "    lambda x: re.sub(r\"https?://[A-Za-z0-9./]+\", \"\", x, flags=re.MULTILINE)\n",
    ")\n",
    "df = df.apply(\n",
    "    lambda x: re.sub(r\"www?://[A-Za-z0-9./]+\", \"\", x, flags=re.MULTILINE)\n",
    ")\n",
    "\n",
    "# Removing RT and link\n",
    "df = df.apply(lambda x: re.sub(r\"\\bRT\\b\", \"\", x).strip())\n",
    "df = df.apply(lambda x: re.sub(r\"\\blink\\b\", \"\", x).strip())\n",
    "\n",
    "# Remove @ and #\n",
    "df = df.apply(lambda x: re.sub(r\"@[A-Za-z0-9_]+\", \"\", x))\n",
    "df = df.apply(lambda x: re.sub(r\"#[A-Za-z0-9_]+\", \"\", x))\n",
    "\n",
    "# Remove stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "df = df.apply(lambda x: \" \".join([x for x in x.split() if x not in stop]))\n",
    "\n",
    "# Remove numbers\n",
    "df = df.apply(lambda x: re.sub(r\"[0-9]+\", \"\", x))\n",
    "\n",
    "# Remove punctuations\n",
    "df = df.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Tokenized\n",
    "tokenizedTweets = [word_tokenize(x) for x in df]\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for tweet in tokenizedTweets:\n",
    "    for word in tweet:\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "\n",
    "processed = tokenizedTweets\n",
    "\n",
    "# Append changed tweet to database\n",
    "final = []\n",
    "for x in range(len(processed)):\n",
    "    final.append(\" \".join(processed[x]))\n",
    "\n",
    "out = pd.DataFrame(final)\n",
    "\n",
    "data[\"changedtweet\"] = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting of Data\n",
    "\n",
    "The data is split into three categories:\n",
    "<br>\n",
    "* Positive\n",
    "* Negative\n",
    "* Neutral\n",
    "<br>\n",
    "\n",
    "The first 5000 tweets in a category's dataset are to be used for testing and training.\n",
    "\n",
    "For each category, the samples are split such that there are 60% tweets for training and 40% tweets for testing.\n",
    "Therefore, for each category, there are:\n",
    "<br>\n",
    "* 3000 tweets for training\n",
    "* 2000 tweets for testing\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5000 samples per label\n",
    "positive = data[data[\"sentiment\"] == 1][:5000]\n",
    "negative = data[data[\"sentiment\"] == -1][:5000]\n",
    "neutral = data[data[\"sentiment\"] == 0][:5000]\n",
    "\n",
    "#The features are extracted from the changed tweet and the target is the sentiment\n",
    "features = [\"changedtweet\"]\n",
    "targets = [\"sentiment\"]\n",
    "\n",
    "#Create an empty Dataframe for features in train and test and sentiment in train and test\n",
    "X_train = pd.DataFrame(columns = features)\n",
    "X_test = pd.DataFrame(columns = features)\n",
    "y_train = pd.DataFrame(columns = targets)\n",
    "y_test = pd.DataFrame(columns = targets)\n",
    "\n",
    "#Set an empty array to append the tweets\n",
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for category in (positive, negative, neutral):\n",
    "    X = category[\"changedtweet\"]\n",
    "    y = category[\"sentiment\"]\n",
    "    Xs_train, Xs_test, ys_train, ys_test = train_test_split(X, y, random_state=0, train_size=0.6) #Split the dataset\n",
    "    \n",
    "    #Append the split data set to the array\n",
    "    X_train_list.append(Xs_train)\n",
    "    X_test_list.append(Xs_test)\n",
    "    y_train_list.append(ys_train)\n",
    "    y_test_list.append(ys_test)\n",
    "\n",
    "#Concat the three categories to create one dataset\n",
    "X_train = pd.concat(X_train_list, ignore_index=True)\n",
    "X_test = pd.concat(X_test_list, ignore_index=True)\n",
    "y_train = pd.concat(y_train_list, ignore_index=True)\n",
    "y_test = pd.concat(y_test_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction (Unigrams)\n",
    "\n",
    "To extract the feature, use a TF-IDF vectorizer.\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to vectorize the tweets (for the Unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer()\n",
      "  (0, 28830)\t0.2887321221437649\n",
      "  (0, 28220)\t0.3655044034158788\n",
      "  (0, 26509)\t0.4365080523212724\n",
      "  (0, 25397)\t0.37763176597422193\n",
      "  (0, 22420)\t0.11040303992949227\n",
      "  (0, 22159)\t0.3425059004694709\n",
      "  (0, 12282)\t0.3835668206604658\n",
      "  (0, 11840)\t0.39613190517300406\n",
      "  (0, 4723)\t0.09056659396775765\n",
      "  (0, 4078)\t0.0902764591644557\n",
      "  (1, 25941)\t0.4695217301918389\n",
      "  (1, 22420)\t0.1500818689863665\n",
      "  (1, 12416)\t0.4822472841578036\n",
      "  (1, 10901)\t0.4006270326558702\n",
      "  (1, 7022)\t0.4592939720250854\n",
      "  (1, 4723)\t0.12311620856718347\n",
      "  (1, 4078)\t0.12272179937731553\n",
      "  (1, 2401)\t0.350415641080777\n",
      "  (2, 25883)\t0.380559044218338\n",
      "  (2, 22876)\t0.24323500970121362\n",
      "  (2, 22420)\t0.08176318416533178\n",
      "  (2, 21922)\t0.3293808252574299\n",
      "  (2, 16977)\t0.26116650552560106\n",
      "  (2, 15074)\t0.333133879641695\n",
      "  (2, 15033)\t0.3721141286152039\n",
      "  :\t:\n",
      "  (8996, 1103)\t0.42191527565196124\n",
      "  (8997, 28034)\t0.1340384827754342\n",
      "  (8997, 25651)\t0.33744751259827394\n",
      "  (8997, 17479)\t0.5032841725949833\n",
      "  (8997, 13181)\t0.4694822589733035\n",
      "  (8997, 10809)\t0.1307619248527065\n",
      "  (8997, 8247)\t0.48303648657683496\n",
      "  (8997, 1606)\t0.3795146678924634\n",
      "  (8998, 29058)\t0.2822269479040056\n",
      "  (8998, 26824)\t0.3206315804763734\n",
      "  (8998, 21254)\t0.24544705669122124\n",
      "  (8998, 21192)\t0.3308191435460486\n",
      "  (8998, 11243)\t0.3124297237308246\n",
      "  (8998, 9476)\t0.28808543897787753\n",
      "  (8998, 6895)\t0.42268857199567744\n",
      "  (8998, 4723)\t0.05765818971274436\n",
      "  (8998, 4078)\t0.057473478697367\n",
      "  (8998, 3959)\t0.42268857199567744\n",
      "  (8998, 3083)\t0.32139274773316623\n",
      "  (8999, 28034)\t0.13632238275546985\n",
      "  (8999, 27915)\t0.6558453179774442\n",
      "  (8999, 22420)\t0.09990830035383627\n",
      "  (8999, 10809)\t0.13298999511563886\n",
      "  (8999, 7022)\t0.30574832534872765\n",
      "  (8999, 2775)\t0.6558453179774442\n"
     ]
    }
   ],
   "source": [
    "#To extract the features use TF-IDF vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(ngram_range=(1,1)) #Unigram\n",
    "print(Tfidf_vect)\n",
    "Tfidf_vect.fit(data['changedtweet']) #Fit the vectorizer to the pre-processed tweets\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train) #Get the vectorized tweets for trainig\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test) #Get the vectorized tweets for testing\n",
    "\n",
    "# features_by_gram = defaultdict(list)\n",
    "# for f, w in zip(Tfidf_vect.get_feature_names(), Tfidf_vect.idf_):\n",
    "#     features_by_gram[len(f.split(' '))].append((f, w))\n",
    "# top_n = 10\n",
    "# for gram, features in features_by_gram.items():\n",
    "#     top_features = sorted(features, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "#     top_features = [f[0] for f in top_features]\n",
    "#     print ('{}-gram top:'.format(gram), top_features)\n",
    "\n",
    "# needs to happen after fit_transform()\n",
    "\n",
    "def display_scores(vectorizer, tfidf_result):\n",
    "    # http://stackoverflow.com/questions/16078015/\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    for item in sorted_scores:\n",
    "        print \"{0:50} Score: {1}\".format(item[0], item[1])\n",
    "\n",
    "display_scores(Tfidf_vect, Train_X_Tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Algorithm for Unigrams\n",
    "\n",
    "The SVM is set up with the hyperparameters:\n",
    "<br>\n",
    "* Kernel- Radial Basis Function\n",
    "* Gamma- 1.3\n",
    "* C- 1000\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report for Unigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.76      0.72      0.74      2000\n",
      "           0       0.65      0.78      0.71      2000\n",
      "           1       0.90      0.77      0.83      2000\n",
      "\n",
      "    accuracy                           0.76      6000\n",
      "   macro avg       0.77      0.76      0.76      6000\n",
      "weighted avg       0.77      0.76      0.76      6000\n",
      "\n",
      "SVM Accuracy Score for Unigrams-> 75.6333\n",
      "SVM Precision Score for Unigrams-> 75.8217\n",
      "SVM Recall Score for Unigrams-> 75.6333\n",
      "SVM F1 Score for Unigrams-> 75.3047\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(kernel='rbf', gamma=1.3, C=1000)\n",
    "SVM.fit(Train_X_Tfidf,y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "print(\"SVM Classification Report for Unigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_SVM))\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score for Unigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_SVM, y_test)*100,4))\n",
    "print(\"SVM Precision Score for Unigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM Recall Score for Unigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM F1 Score for Unigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_SVM, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm for Unigrams\n",
    "\n",
    "The Decision Tree is set up with the hyperparameters:\n",
    "<br>\n",
    "* random state- 0\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report for Unigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.64      0.55      0.59      2000\n",
      "           0       0.55      0.67      0.61      2000\n",
      "           1       0.75      0.71      0.73      2000\n",
      "\n",
      "    accuracy                           0.64      6000\n",
      "   macro avg       0.65      0.64      0.64      6000\n",
      "weighted avg       0.65      0.64      0.64      6000\n",
      "\n",
      "Decision Tree Accuracy Score for Unigrams-> 64.2167\n",
      "Decision Tree Precision Score for Unigrams-> 64.6938\n",
      "Decision Tree Recall Score for Unigrams-> 64.2167\n",
      "Decision Tree F1 Score for Unigrams-> 64.1277\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - Decision Tree\n",
    "# fit the training dataset on the classifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(Train_X_Tfidf, y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_DecisionTree = dt.predict(Test_X_Tfidf)\n",
    "print(\"Decision Tree Classification Report for Unigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_DecisionTree))\n",
    "\n",
    "print(\"Decision Tree Accuracy Score for Unigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_DecisionTree, y_test)*100,4))\n",
    "print(\"Decision Tree Precision Score for Unigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree Recall Score for Unigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree F1 Score for Unigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction (Bigrams)\n",
    "\n",
    "To extract the feature, use TF-IDF vectorizer.\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to vectorize the tweets (for the Bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To extract the features use TF-IDF vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(ngram_range=(2,2)) #Bigrams\n",
    "Tfidf_vect.fit(data['changedtweet']) #Fit the vectorizer to the pre-processed tweets\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train) #Get the vectorized tweets for trainig\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test) #Get the vectorized tweets for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Algorithm for Bigrams\n",
    "\n",
    "The SVM is set up with the hyperparameters:\n",
    "<br>\n",
    "* Kernel- Radial Basis Function\n",
    "* Gamma- 1.3\n",
    "* C- 1000\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report for Bigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      0.68      0.67      2000\n",
      "           0       0.55      0.72      0.63      2000\n",
      "           1       0.97      0.63      0.76      2000\n",
      "\n",
      "    accuracy                           0.68      6000\n",
      "   macro avg       0.73      0.68      0.69      6000\n",
      "weighted avg       0.73      0.68      0.69      6000\n",
      "\n",
      "SVM Accuracy Score for Bigrams-> 67.9\n",
      "SVM Precision Score for Bigrams-> 68.9252\n",
      "SVM Recall Score for Bigrams-> 67.9\n",
      "SVM F1 Score for Bigrams-> 67.1229\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(kernel='rbf', gamma=1.3, C=1000)\n",
    "SVM.fit(Train_X_Tfidf,y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "print(\"SVM Classification Report for Bigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_SVM))\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score for Bigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_SVM, y_test)*100,4))\n",
    "print(\"SVM Precision Score for Bigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM Recall Score for Bigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM F1 Score for Bigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_SVM, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm for Bigrams\n",
    "\n",
    "The Decision Tree is set up with the hyperparameters:\n",
    "<br>\n",
    "* random state- 0\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report for Bigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.57      0.57      2000\n",
      "           0       0.51      0.61      0.56      2000\n",
      "           1       0.81      0.66      0.73      2000\n",
      "\n",
      "    accuracy                           0.61      6000\n",
      "   macro avg       0.63      0.61      0.62      6000\n",
      "weighted avg       0.63      0.61      0.62      6000\n",
      "\n",
      "Decision Tree Accuracy Score for Bigrams-> 61.25\n",
      "Decision Tree Precision Score for Bigrams-> 60.973\n",
      "Decision Tree Recall Score for Bigrams-> 61.25\n",
      "Decision Tree F1 Score for Bigrams-> 60.7093\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - Decision Tree\n",
    "# fit the training dataset on the classifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(Train_X_Tfidf, y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_DecisionTree = dt.predict(Test_X_Tfidf)\n",
    "print(\"Decision Tree Classification Report for Bigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_DecisionTree))\n",
    "\n",
    "print(\"Decision Tree Accuracy Score for Bigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_DecisionTree, y_test)*100,4))\n",
    "print(\"Decision Tree Precision Score for Bigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree Recall Score for Bigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree F1 Score for Bigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_DecisionTree, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction (Trigrams)\n",
    "\n",
    "To extract the feature, use TF-IDF vectorizer.\n",
    "<br>\n",
    "---\n",
    "! Run the cell below to vectorize the tweets (for the Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To extract the features use TF-IDF vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(ngram_range=(3,3)) #Unigram\n",
    "Tfidf_vect.fit(data['changedtweet']) #Fit the vectorizer to the pre-processed tweets\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train) #Get the vectorized tweets for trainig\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test) #Get the vectorized tweets for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Algorithm for Trigrams\n",
    "\n",
    "The SVM is set up with the hyperparameters:\n",
    "<br>\n",
    "* Kernel- Radial Basis Function\n",
    "* Gamma- 1.3\n",
    "* C- 1000\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report for Trigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.63      0.55      0.59      2000\n",
      "           0       0.51      0.78      0.62      2000\n",
      "           1       0.98      0.59      0.73      2000\n",
      "\n",
      "    accuracy                           0.64      6000\n",
      "   macro avg       0.71      0.64      0.65      6000\n",
      "weighted avg       0.71      0.64      0.65      6000\n",
      "\n",
      "SVM Accuracy Score for Trigrams-> 64.0\n",
      "SVM Precision Score for Trigrams-> 67.4366\n",
      "SVM Recall Score for Trigrams-> 64.0\n",
      "SVM F1 Score for Trigrams-> 63.2697\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(kernel='rbf', gamma=1.3, C=1000)\n",
    "SVM.fit(Train_X_Tfidf,y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "print(\"SVM Classification Report for Trigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_SVM))\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score for Trigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_SVM, y_test)*100,4))\n",
    "print(\"SVM Precision Score for Trigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM Recall Score for Trigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_SVM, y_test, average='weighted')*100,4))\n",
    "print(\"SVM F1 Score for Trigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_SVM, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm for Trigrams\n",
    "\n",
    "The Decision Tree is set up with the hyperparameters:\n",
    "<br>\n",
    "* random state- 0\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report for Trigrams\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.39      0.50      2000\n",
      "           0       0.50      0.78      0.61      2000\n",
      "           1       0.82      0.72      0.77      2000\n",
      "\n",
      "    accuracy                           0.63      6000\n",
      "   macro avg       0.67      0.63      0.63      6000\n",
      "weighted avg       0.67      0.63      0.63      6000\n",
      "\n",
      "Decision Tree Accuracy Score for Trigrams-> 63.0167\n",
      "Decision Tree Precision Score for Trigrams-> 69.0803\n",
      "Decision Tree Recall Score for Trigrams-> 63.0167\n",
      "Decision Tree F1 Score for Trigrams-> 63.5244\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - Decision Tree\n",
    "# fit the training dataset on the classifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(Train_X_Tfidf, y_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_DecisionTree = dt.predict(Test_X_Tfidf)\n",
    "print(\"Decision Tree Classification Report for Trigrams\")\n",
    "# Print the classification report\n",
    "print(classification_report(y_test,predictions_DecisionTree))\n",
    "\n",
    "print(\"Decision Tree Accuracy Score for Trigrams-> \", end=\"\")\n",
    "print(round(accuracy_score(predictions_DecisionTree, y_test)*100,4))\n",
    "print(\"Decision Tree Precision Score for Trigrams-> \", end=\"\")\n",
    "print(round(precision_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree Recall Score for Trigrams-> \", end=\"\")\n",
    "print(round(recall_score(predictions_DecisionTree, y_test, average='weighted')*100,4))\n",
    "print(\"Decision Tree F1 Score for Trigrams-> \", end=\"\")\n",
    "print(round(f1_score(predictions_DecisionTree, y_test, average='weighted')*100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
